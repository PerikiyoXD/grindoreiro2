# Testing Guide

> Comprehensive testing strategies for Grindoreiro development

## ğŸ“¦ Overview

This guide covers testing strategies, tools, and best practices for developing and maintaining Grindoreiro. Proper testing ensures code quality, prevents regressions, and enables confident refactoring.

## ğŸ§ª Testing Strategy

### Testing Pyramid

Grindoreiro follows a testing pyramid approach:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  End-to-End Tests (Slow, High Value)
â”‚   E2E       â”‚  - Full pipeline testing
â”‚   Tests     â”‚  - Integration verification
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Integration Tests (Medium Speed, Medium Value)
â”‚ Integration â”‚  - Component interaction
â”‚   Tests     â”‚  - Pipeline flow
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Unit Tests (Fast, High Coverage)
â”‚   Unit      â”‚  - Individual functions
â”‚   Tests     â”‚  - Class methods
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Categories

#### Unit Tests
- Test individual functions and methods
- Mock external dependencies
- Fast execution (< 100ms per test)
- High coverage target (80%+)

#### Integration Tests
- Test component interactions
- Test pipeline execution
- Include real dependencies where possible
- Medium execution time

#### End-to-End Tests
- Test complete workflows
- Use real data and configurations
- Slow execution but high confidence
- Run on CI/CD pipeline

## ğŸ› ï¸ Testing Tools

### Core Testing Framework

```python
# requirements-test.txt
pytest>=7.0.0
pytest-cov>=4.0.0
pytest-mock>=3.10.0
pytest-xdist>=3.0.0  # Parallel test execution
pytest-html>=3.1.0   # HTML test reports
```

### Additional Testing Tools

```python
# For more comprehensive testing
hypothesis>=6.0.0    # Property-based testing
faker>=15.0.0        # Fake data generation
freezegun>=1.2.0     # Time mocking
responses>=0.23.0    # HTTP mocking
```

## ğŸ“ Writing Unit Tests

### Basic Unit Test Structure

```python
# tests/test_core.py
import pytest
from unittest.mock import Mock, patch
from grindoreiro.core import Config

class TestConfig:
    """Test cases for Config class."""

    def test_config_initialization_default(self):
        """Test Config initialization with default values."""
        config = Config()

        assert config.input_dir == "./data/samples"
        assert config.output_dir == "./data/output"
        assert config.max_workers == 4
        assert config.verbose is False

    def test_config_initialization_custom(self):
        """Test Config initialization with custom values."""
        config = Config(
            input_dir="/custom/input",
            output_dir="/custom/output",
            max_workers=8,
            verbose=True
        )

        assert config.input_dir == "/custom/input"
        assert config.output_dir == "/custom/output"
        assert config.max_workers == 8
        assert config.verbose is True

    @patch.dict('os.environ', {'GRINDOREIRO_INPUT_DIR': '/env/input'})
    def test_config_from_env(self):
        """Test loading config from environment variables."""
        config = Config.from_env()

        assert config.input_dir == "/env/input"
        # Other values should be defaults
        assert config.max_workers == 4

    def test_config_from_file_valid(self, tmp_path):
        """Test loading config from valid YAML file."""
        config_file = tmp_path / "config.yaml"
        config_file.write_text("""
grindoreiro:
  input_dir: "/yaml/input"
  max_workers: 6
  verbose: true
""")

        config = Config.from_file(str(config_file))

        assert config.input_dir == "/yaml/input"
        assert config.max_workers == 6
        assert config.verbose is True

    def test_config_from_file_invalid(self, tmp_path):
        """Test loading config from invalid file."""
        config_file = tmp_path / "invalid.yaml"
        config_file.write_text("invalid: yaml: content: [")

        with pytest.raises(Exception):
            Config.from_file(str(config_file))
```

### Testing Pipeline Steps

```python
# tests/test_pipeline_steps.py
import pytest
from unittest.mock import Mock, patch
from pathlib import Path
from grindoreiro.pipeline_steps import ExtractionStep, PipelineStep
from grindoreiro.core import Config

class TestExtractionStep:
    """Test cases for ExtractionStep."""

    def test_step_inheritance(self):
        """Test that ExtractionStep inherits from PipelineStep."""
        step = ExtractionStep()
        assert isinstance(step, PipelineStep)
        assert step.name == "ExtractionStep"

    def test_extract_zip_success(self, tmp_path):
        """Test successful ZIP extraction."""
        # Create a test ZIP file
        zip_file = tmp_path / "test.zip"
        # Create ZIP with test content

        output_dir = tmp_path / "extracted"
        output_dir.mkdir()

        step = ExtractionStep()
        extracted_files = step._extract_zip(zip_file, output_dir)

        assert len(extracted_files) > 0
        assert all(f.exists() for f in extracted_files)

    def test_extract_zip_nonexistent(self, tmp_path):
        """Test ZIP extraction with non-existent file."""
        zip_file = tmp_path / "nonexistent.zip"
        output_dir = tmp_path / "extracted"
        output_dir.mkdir()

        step = ExtractionStep()

        with pytest.raises(FileNotFoundError):
            step._extract_zip(zip_file, output_dir)

    @patch('zipfile.ZipFile')
    def test_extract_zip_mock(self, mock_zipfile, tmp_path):
        """Test ZIP extraction with mocked ZipFile."""
        # Setup mock
        mock_zip = Mock()
        mock_zipfile.return_value.__enter__.return_value = mock_zip
        mock_zip.namelist.return_value = ['file1.txt', 'file2.txt']

        zip_file = tmp_path / "test.zip"
        output_dir = tmp_path / "extracted"
        output_dir.mkdir()

        step = ExtractionStep()
        extracted_files = step._extract_zip(zip_file, output_dir)

        mock_zipfile.assert_called_once_with(zip_file, 'r')
        mock_zip.extractall.assert_called_once_with(output_dir)
        assert len(extracted_files) == 2

    def test_process_context_update(self, tmp_path):
        """Test that process method updates context correctly."""
        # Create test file
        test_file = tmp_path / "test.txt"
        test_file.write_text("test content")

        step = ExtractionStep()
        context = {
            'file_path': test_file,
            'extraction_dir': tmp_path / "extracted"
        }

        result_context = step.process(context)

        # Context should be updated with extraction results
        assert 'extracted_files' in result_context
        assert isinstance(result_context['extracted_files'], list)
```

### Testing with Fixtures

```python
# tests/conftest.py
import pytest
from pathlib import Path
import tempfile
import zipfile
import os

@pytest.fixture
def temp_dir():
    """Provide a temporary directory for tests."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)

@pytest.fixture
def sample_zip_file(temp_dir):
    """Create a sample ZIP file for testing."""
    zip_path = temp_dir / "sample.zip"

    with zipfile.ZipFile(zip_path, 'w') as zf:
        # Add some test files
        zf.writestr('file1.txt', 'content 1')
        zf.writestr('file2.txt', 'content 2')
        zf.writestr('subdir/file3.txt', 'content 3')

    return zip_path

@pytest.fixture
def sample_exe_file(temp_dir):
    """Create a sample executable file for testing."""
    exe_path = temp_dir / "sample.exe"

    # Create a minimal PE file structure
    pe_header = b'MZ\x90\x00\x03\x00\x00\x00\x04\x00\x00\x00\xff\xff\x00\x00'
    pe_header += b'\xb8\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x00'
    pe_header += b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
    pe_header += b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'

    exe_path.write_bytes(pe_header)
    return exe_path

@pytest.fixture
def config():
    """Provide a default Config instance."""
    from grindoreiro.core import Config
    return Config()

# tests/test_with_fixtures.py
class TestWithFixtures:
    """Test cases using fixtures."""

    def test_zip_extraction_with_fixture(self, sample_zip_file, temp_dir):
        """Test ZIP extraction using fixture."""
        from grindoreiro.pipeline_steps import ExtractionStep

        output_dir = temp_dir / "extracted"
        output_dir.mkdir()

        step = ExtractionStep()
        extracted_files = step._extract_zip(sample_zip_file, output_dir)

        assert len(extracted_files) == 3
        assert all(f.exists() for f in extracted_files)

    def test_file_type_detection(self, sample_exe_file):
        """Test file type detection with fixture."""
        # Test file type detection logic
        with open(sample_exe_file, 'rb') as f:
            header = f.read(2)

        assert header == b'MZ'  # PE file signature
```

## ğŸ”— Integration Testing

### Pipeline Integration Tests

```python
# tests/test_pipeline_integration.py
import pytest
from pathlib import Path
from grindoreiro.pipeline import Pipeline
from grindoreiro.core import Config
from grindoreiro.pipeline_steps import (
    FileValidationStep,
    HashCalculationStep,
    ExtractionStep
)

class TestPipelineIntegration:
    """Integration tests for pipeline execution."""

    def test_basic_pipeline_execution(self, sample_zip_file, temp_dir):
        """Test complete pipeline execution."""
        config = Config()
        pipeline = Pipeline(config)

        # Add basic steps
        pipeline.add_step(FileValidationStep())
        pipeline.add_step(HashCalculationStep())
        pipeline.add_step(ExtractionStep())

        context = {'file_path': sample_zip_file}
        result = pipeline.run(context)

        # Verify pipeline completed successfully
        assert 'file_path' in result
        assert 'file_hashes' in result
        assert 'extracted_files' in result

        # Verify extraction worked
        extracted_files = result['extracted_files']
        assert len(extracted_files) > 0

    def test_pipeline_error_handling(self, temp_dir):
        """Test pipeline error handling."""
        config = Config()
        pipeline = Pipeline(config)

        # Add steps that might fail
        pipeline.add_step(FileValidationStep())
        pipeline.add_step(ExtractionStep())

        # Test with non-existent file
        nonexistent_file = temp_dir / "nonexistent.zip"
        context = {'file_path': nonexistent_file}

        result = pipeline.run(context)

        # Should handle error gracefully
        assert 'error' in result or 'extraction_error' in result

    def test_pipeline_step_ordering(self, sample_zip_file, temp_dir):
        """Test that pipeline steps execute in correct order."""
        config = Config()
        pipeline = Pipeline(config)

        # Create custom steps to track execution order
        execution_order = []

        class TrackingStep:
            def __init__(self, name):
                self.name = name

            def process(self, context):
                execution_order.append(self.name)
                return context

        # Add tracking steps
        pipeline.add_step(TrackingStep("step1"))
        pipeline.add_step(TrackingStep("step2"))
        pipeline.add_step(TrackingStep("step3"))

        context = {'file_path': sample_zip_file}
        pipeline.run(context)

        # Verify execution order
        assert execution_order == ["step1", "step2", "step3"]

    @pytest.mark.slow
    def test_large_file_processing(self, temp_dir):
        """Test processing of large files."""
        # Create a large test file
        large_file = temp_dir / "large.zip"
        # Create large ZIP file for testing

        config = Config(max_workers=1)  # Single thread for predictable testing
        pipeline = Pipeline(config)

        # Add processing steps
        pipeline.add_step(FileValidationStep())
        pipeline.add_step(ExtractionStep())

        context = {'file_path': large_file}

        # Should complete without memory issues
        result = pipeline.run(context)

        assert 'extracted_files' in result
```

### Processor Integration Tests

```python
# tests/test_processor_integration.py
import pytest
from pathlib import Path
from grindoreiro.processor import Processor
from grindoreiro.core import Config

class TestProcessorIntegration:
    """Integration tests for Processor class."""

    def test_single_file_processing(self, sample_zip_file, temp_dir):
        """Test processing a single file."""
        config = Config(
            input_dir=str(temp_dir),
            output_dir=str(temp_dir / "output"),
            temp_dir=str(temp_dir / "temp")
        )

        processor = Processor(config)
        result = processor.process_file(sample_zip_file)

        assert result['success'] is True
        assert 'processing_time' in result
        assert result['processing_time'] > 0
        assert 'file_path' in result

    def test_batch_file_processing(self, temp_dir):
        """Test processing multiple files."""
        # Create multiple test files
        test_files = []
        for i in range(3):
            zip_file = temp_dir / f"test_{i}.zip"
            # Create test ZIP files
            test_files.append(zip_file)

        config = Config(
            input_dir=str(temp_dir),
            output_dir=str(temp_dir / "output"),
            max_workers=2
        )

        processor = Processor(config)
        results = processor.process_batch(test_files)

        assert len(results) == 3
        assert all(result['success'] for result in results)
        assert all('processing_time' in result for result in results)

    def test_error_recovery(self, temp_dir):
        """Test error recovery in batch processing."""
        # Create mix of valid and invalid files
        valid_file = temp_dir / "valid.zip"
        # Create valid ZIP

        invalid_file = temp_dir / "invalid.zip"
        invalid_file.write_bytes(b"not a zip file")

        config = Config(
            input_dir=str(temp_dir),
            output_dir=str(temp_dir / "output")
        )

        processor = Processor(config)
        results = processor.process_batch([valid_file, invalid_file])

        assert len(results) == 2

        # One should succeed, one should fail
        success_count = sum(1 for r in results if r['success'])
        assert success_count >= 0  # At least some should work

        # All should have processing time
        assert all('processing_time' in r for r in results)
```

## ğŸƒ Running Tests

### Basic Test Execution

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_core.py

# Run specific test class
pytest tests/test_core.py::TestConfig

# Run specific test method
pytest tests/test_core.py::TestConfig::test_config_initialization_default

# Run tests with verbose output
pytest -v

# Run tests with coverage
pytest --cov=grindoreiro --cov-report=html
```

### Advanced Test Execution

```bash
# Run tests in parallel
pytest -n auto

# Run only fast tests
pytest -m "not slow"

# Run tests with different Python versions
tox

# Generate HTML test report
pytest --html=report.html --self-contained-html

# Run tests and stop on first failure
pytest -x

# Run tests with detailed output for failures
pytest -v --tb=short
```

### Test Configuration

```ini
# pytest.ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    -v
    --strict-markers
    --disable-warnings
    --cov=grindoreiro
    --cov-report=html
    --cov-report=term
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
```

## ğŸ“Š Test Coverage

### Coverage Configuration

```ini
# .coveragerc
[run]
source = grindoreiro
omit =
    */tests/*
    */venv/*
    */__pycache__/*
    setup.py

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

[html]
directory = htmlcov
```

### Coverage Goals

- **Unit Tests**: 80%+ coverage
- **Integration Tests**: Cover critical paths
- **End-to-End Tests**: Cover main workflows

### Coverage Analysis

```python
# tests/test_coverage.py
import pytest
import coverage

def test_coverage_threshold():
    """Test that coverage meets minimum threshold."""
    cov = coverage.Coverage()
    cov.load()

    # Get coverage data
    total_coverage = cov.report()

    # Assert minimum coverage
    assert total_coverage >= 80.0, f"Coverage {total_coverage}% is below 80% threshold"
```

## ğŸ§ª Property-Based Testing

### Using Hypothesis

```python
# tests/test_property_based.py
import pytest
from hypothesis import given, strategies as st
from grindoreiro.core import Config

class TestPropertyBased:
    """Property-based tests using Hypothesis."""

    @given(
        input_dir=st.text(min_size=1, max_size=100),
        output_dir=st.text(min_size=1, max_size=100),
        max_workers=st.integers(min_value=1, max_value=16),
        verbose=st.booleans()
    )
    def test_config_properties(self, input_dir, output_dir, max_workers, verbose):
        """Test Config properties with various inputs."""
        config = Config(
            input_dir=input_dir,
            output_dir=output_dir,
            max_workers=max_workers,
            verbose=verbose
        )

        # Properties that should always hold
        assert isinstance(config.input_dir, str)
        assert isinstance(config.output_dir, str)
        assert isinstance(config.max_workers, int)
        assert isinstance(config.verbose, bool)

        assert config.max_workers >= 1
        assert len(config.input_dir) > 0
        assert len(config.output_dir) > 0

    @given(st.lists(st.text(min_size=1, max_size=50), min_size=1, max_size=10))
    def test_string_list_processing(self, string_list):
        """Test string processing with various inputs."""
        from grindoreiro.analyzer import StringAnalyzer

        analyzer = StringAnalyzer()

        # Process strings
        result = analyzer.analyze_strings(string_list)

        # Properties
        assert isinstance(result, dict)
        assert 'total_strings' in result
        assert result['total_strings'] == len(string_list)
        assert 'unique_strings' in result
        assert result['unique_strings'] <= len(string_list)
```

## ğŸ”§ Mocking and Fixtures

### Advanced Mocking

```python
# tests/test_mocking.py
import pytest
from unittest.mock import Mock, MagicMock, patch, call
from pathlib import Path
from grindoreiro.pipeline_steps import ExtractionStep

class TestAdvancedMocking:
    """Advanced mocking techniques."""

    def test_mock_method_calls(self):
        """Test mocking method calls and verifying interactions."""
        step = ExtractionStep()

        # Mock the _extract_zip method
        with patch.object(step, '_extract_zip') as mock_extract:
            mock_extract.return_value = ['file1.txt', 'file2.txt']

            # Create mock file path
            mock_file = Mock(spec=Path)
            mock_file.suffix.lower.return_value = '.zip'

            # Call the method
            result = step._extract_zip(mock_file, Mock())

            # Verify the call
            mock_extract.assert_called_once_with(mock_file, Mock())
            assert result == ['file1.txt', 'file2.txt']

    def test_mock_file_operations(self, tmp_path):
        """Test mocking file operations."""
        test_file = tmp_path / "test.txt"
        test_file.write_text("test content")

        with patch('builtins.open', create=True) as mock_open:
            mock_file = Mock()
            mock_file.read.return_value = b"mocked content"
            mock_open.return_value.__enter__.return_value = mock_file

            # Test file reading
            with open(test_file, 'rb') as f:
                content = f.read()

            assert content == b"mocked content"
            mock_open.assert_called_once_with(test_file, 'rb')

    def test_mock_context_manager(self):
        """Test mocking context managers."""
        with patch('zipfile.ZipFile') as mock_zipfile:
            mock_zip = Mock()
            mock_zipfile.return_value.__enter__.return_value = mock_zip
            mock_zip.namelist.return_value = ['file1.txt']

            # Use in context
            with mock_zipfile('test.zip', 'r') as zf:
                files = zf.namelist()

            assert files == ['file1.txt']
            mock_zipfile.assert_called_once_with('test.zip', 'r')

    def test_partial_mock(self):
        """Test partial mocking of real objects."""
        step = ExtractionStep()

        # Mock only specific method
        with patch.object(step, '_create_extraction_dir') as mock_create_dir:
            mock_create_dir.return_value = Path('/mocked/path')

            # Call method that uses the mocked method
            result = step._create_extraction_dir(Path('test.zip'))

            assert result == Path('/mocked/path')
            mock_create_dir.assert_called_once()
```

### Custom Fixtures

```python
# tests/conftest.py (continued)
import pytest
from unittest.mock import Mock
from grindoreiro.core import Config
from grindoreiro.pipeline import Pipeline

@pytest.fixture
def mock_config():
    """Provide a mocked Config instance."""
    config = Mock(spec=Config)
    config.input_dir = "/mock/input"
    config.output_dir = "/mock/output"
    config.max_workers = 2
    config.verbose = False
    return config

@pytest.fixture
def mock_pipeline(mock_config):
    """Provide a mocked Pipeline instance."""
    pipeline = Mock(spec=Pipeline)
    pipeline.config = mock_config
    pipeline.run.return_value = {'success': True}
    return pipeline

@pytest.fixture
def mock_file_path(tmp_path):
    """Provide a mock file path with content."""
    file_path = tmp_path / "mock_file.zip"

    # Create a simple mock file
    file_path.write_bytes(b"mock zip content")

    # Mock the Path object
    mock_path = Mock(spec=Path)
    mock_path.__str__ = Mock(return_value=str(file_path))
    mock_path.exists.return_value = True
    mock_path.stat.return_value = Mock(st_size=1024)
    mock_path.suffix = '.zip'

    return mock_path

@pytest.fixture(scope="session")
def test_data_dir():
    """Provide a session-scoped test data directory."""
    import tempfile
    from pathlib import Path

    temp_dir = Path(tempfile.mkdtemp())

    # Create test data files
    (temp_dir / "sample1.exe").write_bytes(b"MZ\x90\x00")
    (temp_dir / "sample2.zip").write_bytes(b"PK\x03\x04")

    yield temp_dir

    # Cleanup
    import shutil
    shutil.rmtree(temp_dir)
```

## ğŸš€ Continuous Integration

### GitHub Actions Configuration

```yaml
# .github/workflows/test.yml
name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, "3.10", "3.11"]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run tests
      run: |
        pytest --cov=grindoreiro --cov-report=xml

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: true

  integration-test:
    runs-on: ubuntu-latest
    needs: test

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run integration tests
      run: |
        pytest tests/test_integration.py -v

  e2e-test:
    runs-on: ubuntu-latest
    needs: integration-test

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run end-to-end tests
      run: |
        pytest tests/test_e2e.py -v --tb=short
```

### Pre-commit Hooks

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.1.0
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
        args: ["--profile", "black"]

  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args: ["--max-line-length=88", "--extend-ignore=E203,W503"]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.0.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
```

## ğŸ“Š Test Reporting

### HTML Test Reports

```python
# Generate HTML test report
pytest --html=report.html --self-contained-html

# View report in browser
# open report.html
```

### Coverage Reports

```python
# Generate coverage report
pytest --cov=grindoreiro --cov-report=html

# View HTML coverage report
# open htmlcov/index.html
```

### Custom Test Reporting

```python
# tests/test_reporting.py
import pytest
import json
from pathlib import Path

@pytest.fixture(autouse=True)
def generate_test_report(request):
    """Generate custom test report."""
    yield

    # After all tests complete
    if request.config.getoption("--generate-report"):
        report_data = {
            'test_session': str(request.session),
            'passed': len(request.session.results) - len(request.session.results.failed),
            'failed': len(request.session.results.failed),
            'errors': len(request.session.results.errors),
            'skipped': len(request.session.results.skipped),
        }

        report_file = Path('test_report.json')
        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2)
```

## ğŸ¯ Best Practices

### Test Organization

1. **Test File Naming**: `test_*.py` for test files
2. **Test Class Naming**: `Test*` for test classes
3. **Test Method Naming**: `test_*` for test methods
4. **Descriptive Names**: Use descriptive test names
5. **Single Responsibility**: Each test should test one thing

### Test Quality

1. **Arrange-Act-Assert**: Follow AAA pattern
2. **Independent Tests**: Tests should not depend on each other
3. **Fast Tests**: Keep unit tests fast
4. **Realistic Data**: Use realistic test data
5. **Edge Cases**: Test edge cases and error conditions

### Mocking Guidelines

1. **Minimal Mocking**: Mock only external dependencies
2. **Real Objects**: Use real objects when possible
3. **Verify Interactions**: Verify important method calls
4. **Readable Mocks**: Make mocks easy to understand
5. **Consistent Setup**: Use fixtures for common mock setup

### CI/CD Integration

1. **Fast Feedback**: Run fast tests first
2. **Parallel Execution**: Run tests in parallel
3. **Coverage Requirements**: Enforce coverage thresholds
4. **Multiple Environments**: Test on multiple Python versions
5. **Artifact Storage**: Store test reports and coverage

---

ğŸ’¡ **Pro Tip**: Write tests before implementing features (TDD). This ensures testable code design and prevents regressions. Use descriptive test names that explain what the test verifies.</content>
<parameter name="filePath">c:\Users\pxd\Desktop\grindoreiro\docs\developer-guides\testing.md
